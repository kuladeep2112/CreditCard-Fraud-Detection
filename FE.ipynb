{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from scipy.stats import vonmises\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pickled data from EDA file\n",
    "df_train = pd.read_pickle('df_train.pkl')\n",
    "df_test  = pd.read_pickle('df_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features and Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As per Competition host data description https://www.kaggle.com/competitions/ieee-fraud-detection/discussion/101203\n",
    "cat_features = ['ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', \n",
    "               'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain', 'M1',\n",
    "               'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'DeviceType', 'DeviceInfo',\n",
    "               'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20',\n",
    "               'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30',\n",
    "               'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = ['TransactionID', 'isFraud']\n",
    "num_features = [col_name for col_name in df_train.columns if (col_name not in cat_features) & (col_name not in exclude)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "1) When Constructuin a credit card fraud detection model, it is important how to extract the right features from the transactional data. This is usually done by aggregating the transactions inoreder to observe the spending behavioral patterns of the customers.\n",
    "2) We aggregated transactions to capture consumer buying behavior prior to each transaction and used these aggregations for model estimation to identify fraudulent transactions.\n",
    "3) One difficulty with analysis of credit card fraud is that perpetrators do not\n",
    "usually carry on a single fraudulent transaction. \n",
    "4) Analyzing fraud from the perspective of a ‘‘one by one’’ transaction omits the idea\n",
    "of clustering that is inherent of credit card fraud actions.\n",
    "5) Perpetrators usually produce a group of fraudulent transactions. We argue that analyzing the aggregated behavior is essential to improve credit card fraud detection rates.\n",
    "6) From Top 20 features of below journal we adopted 13 features which are relevent for our dataset \n",
    "https://www.researchgate.net/publication/335139727_Predicting_Credit_Card_Transaction_Fraud_Using_Machine_Learning_Algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/brodzik/ieee-cis-fraud-detection/notebook?scriptVersionId=21213096\n",
    "for df in [df_train, df_test]:\n",
    "    df[\"day_of_week\"] = np.floor((df[\"TransactionDT\"] / (3600 * 24) - 1) % 7).astype(int)\n",
    "    \n",
    "    df[\"TransactionAmt_int\"] = df[\"TransactionAmt\"].astype(int)\n",
    "    df[\"TransactionAmt_dec\"] = (1000 * (df[\"TransactionAmt\"] - df[\"TransactionAmt_int\"])).astype(int)\n",
    "    \n",
    "        \n",
    "    for a, b in itertools.combinations([\"card1\", \"card2\", \"card3\", \"card4\", \"card5\", \"addr1\", \"addr2\", \"dist1\", \"dist2\"], 2):\n",
    "        df[\"{}__{}\".format(a, b)] = df[a].astype(str) + \"__\" + df[b].astype(str)\n",
    "    \n",
    "    df[[\"P_emaildomain_0\", \"P_emaildomain_1\", \"P_emaildomain_2\"]] = df[\"P_emaildomain\"].str.split(\".\", expand=True)\n",
    "    df[[\"R_emaildomain_0\", \"R_emaildomain_1\", \"R_emaildomain_2\"]] = df[\"R_emaildomain\"].str.split(\".\", expand=True)\n",
    "    df[[\"operating_system_0\", \"operating_system_1\", \"operating_system_2\", \"operating_system_3\"]] = df[\"id_30\"].str.split(\" \", expand=True)\n",
    "    df[[\"browser_0\", \"browser_1\", \"browser_2\", \"browser_3\"]] = df[\"id_31\"].str.split(\" \", expand=True)\n",
    "    df[[\"resolution_width\", \"resolution_height\"]] = df[\"id_33\"].str.split(\"x\", expand=True)\n",
    "    df[[\"DeviceInfo_0\", \"DeviceInfo_1\", \"DeviceInfo_2\", \"DeviceInfo_3\", \"DeviceInfo_4\"]] = df[\"DeviceInfo\"].str.split(r\"[ -/_]\", expand=True)[[0, 1, 2, 3, 4]]\n",
    "\n",
    "    df[\"TransactionAmt_to_mean_card1\"] = df[\"TransactionAmt\"] / df.groupby([\"card1\"])[\"TransactionAmt\"].transform(\"mean\")\n",
    "    df[\"TransactionAmt_to_mean_card4\"] = df[\"TransactionAmt\"] / df.groupby([\"card4\"])[\"TransactionAmt\"].transform(\"mean\")\n",
    "    df[\"TransactionAmt_to_std_card1\"] = df[\"TransactionAmt\"] / df.groupby([\"card1\"])[\"TransactionAmt\"].transform(\"std\")\n",
    "    df[\"TransactionAmt_to_std_card4\"] = df[\"TransactionAmt\"] / df.groupby([\"card4\"])[\"TransactionAmt\"].transform(\"std\")\n",
    "\n",
    "    df[\"id_02_to_mean_card1\"] = df[\"id_02\"] / df.groupby([\"card1\"])[\"id_02\"].transform(\"mean\")\n",
    "    df[\"id_02_to_mean_card4\"] = df[\"id_02\"] / df.groupby([\"card4\"])[\"id_02\"].transform(\"mean\")\n",
    "    df[\"id_02_to_std_card1\"] = df[\"id_02\"] / df.groupby([\"card1\"])[\"id_02\"].transform(\"std\")\n",
    "    df[\"id_02_to_std_card4\"] = df[\"id_02\"] / df.groupby([\"card4\"])[\"id_02\"].transform(\"std\")\n",
    "\n",
    "    df[\"D15_to_mean_card1\"] = df[\"D15\"] / df.groupby([\"card1\"])[\"D15\"].transform(\"mean\")\n",
    "    df[\"D15_to_mean_card4\"] = df[\"D15\"] / df.groupby([\"card4\"])[\"D15\"].transform(\"mean\")\n",
    "    df[\"D15_to_std_card1\"] = df[\"D15\"] / df.groupby([\"card1\"])[\"D15\"].transform(\"std\")\n",
    "    df[\"D15_to_std_card4\"] = df[\"D15\"] / df.groupby([\"card4\"])[\"D15\"].transform(\"std\")\n",
    "\n",
    "    df[\"D15_to_mean_addr1\"] = df[\"D15\"] / df.groupby([\"addr1\"])[\"D15\"].transform(\"mean\")\n",
    "    df[\"D15_to_mean_addr2\"] = df[\"D15\"] / df.groupby([\"addr2\"])[\"D15\"].transform(\"mean\")\n",
    "    df[\"D15_to_std_addr1\"] = df[\"D15\"] / df.groupby([\"addr1\"])[\"D15\"].transform(\"std\")\n",
    "    df[\"D15_to_std_addr2\"] = df[\"D15\"] / df.groupby([\"addr2\"])[\"D15\"].transform(\"std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_train.columns[212:]:\n",
    "    if i[:3]=='Tra' or i[:3]=='id_' or i[:3]=='D15' or i[:3]=='day':\n",
    "        num_features.append(i)\n",
    "    else:\n",
    "        cat_features.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/cdeotte/xgb-fraud-with-magic-0-9600#The-Magic-Feature---UID\n",
    "# FREQUENCY ENCODE TOGETHER\n",
    "def encode_FE(df1, df2, cols):\n",
    "    for col in cols:\n",
    "        vc = df1[col].value_counts(dropna=True, normalize=True).to_dict()\n",
    "        vc1 = df2[col].value_counts(dropna=True, normalize=True).to_dict()\n",
    "        vc[-1] = -1\n",
    "        nm = col+'_FE'\n",
    "        df1[nm] = df1[col].map(vc)\n",
    "        df1[nm] = df1[nm].astype('float32')\n",
    "        df2[nm] = df2[col].map(vc1)\n",
    "        df2[nm] = df2[nm].astype('float32')\n",
    "\n",
    "# LABEL ENCODE\n",
    "def encode_LE(col,train=df_train,test=df_test):\n",
    "    df_comb,_ = train[col].factorize(sort=True)\n",
    "    df_comb1,_ = test[col].factorize(sort=True)\n",
    "    nm = col\n",
    "    if df_comb.max()>32000: \n",
    "        train[nm] = df_comb[:len(train)].astype('int32')\n",
    "        test[nm] = df_comb1[len(train):].astype('int32')\n",
    "    else:\n",
    "        train[nm] = df_comb[:len(train)].astype('int16')\n",
    "        test[nm] = df_comb1[len(train):].astype('int16')\n",
    "    del df_comb; x=gc.collect()\n",
    "# COMBINE FEATURES\n",
    "def encode_CB(col1,col2):\n",
    "    nm = col1+'_'+col2\n",
    "    df_train[nm] = df_train[col1].astype(str)+'_'+df_train[col2].astype(str)\n",
    "    df_test[nm]= df_test[col1].astype(str)+'_'+df_test[col2].astype(str) \n",
    "    encode_LE(nm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_AG(main_columns, uids, aggregations=['mean'], train_df=df_train, test_df=df_test, \n",
    "              fillna=True, usena=False):\n",
    "    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = main_column+'_'+col+'_'+agg_type\n",
    "                temp_df =train_df[[col, main_column]]\n",
    "                temp_df1=test_df[[col,main_column]]\n",
    "                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "                if usena: temp_df1.loc[temp_df[main_column]==-1,main_column] = np.nan\n",
    "                temp_df1 = temp_df1.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df1.index = list(temp_df1[col])\n",
    "                temp_df1 = temp_df1[new_col_name].to_dict()\n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df1).astype('float32')\n",
    "                \n",
    "                if fillna:\n",
    "                    train_df[new_col_name].fillna(-1,inplace=True)\n",
    "                    test_df[new_col_name].fillna(-1,inplace=True)\n",
    "# GROUP AGGREGATION NUNIQUE\n",
    "def encode_AG2(main_columns, uids, train_df=df_train, test_df=df_test):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n",
    "            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n",
    "            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n",
    "            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,16):\n",
    "    if i in [1,2,3,5,9]: continue\n",
    "    df_train['D'+str(i)] =  df_train['D'+str(i)] - df_train.TransactionDT/np.float32(24*60*60)\n",
    "    df_test['D'+str(i)]  = df_test['D'+str(i)] - df_test.TransactionDT/np.float32(24*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREQUENCY ENCODE: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN\n",
    "encode_FE(df_train,df_test,['addr1','card1','card2','card3','P_emaildomain'])\n",
    "# COMBINE COLUMNS CARD1+ADDR1, CARD1+ADDR1+P_EMAILDOMAIN\n",
    "encode_CB('card1','addr1')\n",
    "encode_CB('card1_addr1','P_emaildomain')\n",
    "# FREQUENCY ENOCDE\n",
    "encode_FE(df_train,df_test,['card1_addr1','card1_addr1_P_emaildomain'])\n",
    "# GROUP AGGREGATE\n",
    "encode_AG(['TransactionAmt','D9','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],usena=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['uid'] = df_train.card1_addr1.astype(str)+'_'+np.floor(df_train.day-df_train.D1).astype(str)\n",
    "df_test['uid'] = df_test.card1_addr1.astype(str)+'_'+np.floor(df_test.day-df_test.D1).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TransactionID',\n",
       " 'isFraud',\n",
       " 'TransactionDT',\n",
       " 'TransactionAmt',\n",
       " 'ProductCD',\n",
       " 'card1',\n",
       " 'card2',\n",
       " 'card3',\n",
       " 'card4',\n",
       " 'card5',\n",
       " 'card6',\n",
       " 'addr1',\n",
       " 'addr2',\n",
       " 'dist1',\n",
       " 'dist2',\n",
       " 'P_emaildomain',\n",
       " 'R_emaildomain',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C3',\n",
       " 'C4',\n",
       " 'C5',\n",
       " 'C6',\n",
       " 'C7',\n",
       " 'C8',\n",
       " 'C9',\n",
       " 'C10',\n",
       " 'C11',\n",
       " 'C12',\n",
       " 'C13',\n",
       " 'C14',\n",
       " 'D1',\n",
       " 'D2',\n",
       " 'D3',\n",
       " 'D4',\n",
       " 'D5',\n",
       " 'D6',\n",
       " 'D7',\n",
       " 'D8',\n",
       " 'D9',\n",
       " 'D10',\n",
       " 'D11',\n",
       " 'D12',\n",
       " 'D13',\n",
       " 'D14',\n",
       " 'D15',\n",
       " 'M1',\n",
       " 'M2',\n",
       " 'M3',\n",
       " 'M4',\n",
       " 'M5',\n",
       " 'M6',\n",
       " 'M7',\n",
       " 'M8',\n",
       " 'M9',\n",
       " 'V1',\n",
       " 'V2',\n",
       " 'V3',\n",
       " 'V4',\n",
       " 'V6',\n",
       " 'V7',\n",
       " 'V8',\n",
       " 'V9',\n",
       " 'V10',\n",
       " 'V12',\n",
       " 'V14',\n",
       " 'V15',\n",
       " 'V17',\n",
       " 'V19',\n",
       " 'V23',\n",
       " 'V24',\n",
       " 'V25',\n",
       " 'V26',\n",
       " 'V27',\n",
       " 'V35',\n",
       " 'V37',\n",
       " 'V44',\n",
       " 'V46',\n",
       " 'V47',\n",
       " 'V53',\n",
       " 'V55',\n",
       " 'V56',\n",
       " 'V61',\n",
       " 'V66',\n",
       " 'V67',\n",
       " 'V75',\n",
       " 'V77',\n",
       " 'V78',\n",
       " 'V82',\n",
       " 'V86',\n",
       " 'V87',\n",
       " 'V95',\n",
       " 'V98',\n",
       " 'V99',\n",
       " 'V100',\n",
       " 'V104',\n",
       " 'V107',\n",
       " 'V108',\n",
       " 'V109',\n",
       " 'V110',\n",
       " 'V111',\n",
       " 'V112',\n",
       " 'V114',\n",
       " 'V115',\n",
       " 'V116',\n",
       " 'V117',\n",
       " 'V118',\n",
       " 'V120',\n",
       " 'V121',\n",
       " 'V122',\n",
       " 'V123',\n",
       " 'V124',\n",
       " 'V125',\n",
       " 'V129',\n",
       " 'V130',\n",
       " 'V131',\n",
       " 'V135',\n",
       " 'V136',\n",
       " 'V138',\n",
       " 'V139',\n",
       " 'V141',\n",
       " 'V144',\n",
       " 'V146',\n",
       " 'V148',\n",
       " 'V161',\n",
       " 'V166',\n",
       " 'V169',\n",
       " 'V170',\n",
       " 'V171',\n",
       " 'V172',\n",
       " 'V173',\n",
       " 'V174',\n",
       " 'V175',\n",
       " 'V176',\n",
       " 'V184',\n",
       " 'V185',\n",
       " 'V186',\n",
       " 'V187',\n",
       " 'V188',\n",
       " 'V194',\n",
       " 'V200',\n",
       " 'V205',\n",
       " 'V208',\n",
       " 'V209',\n",
       " 'V210',\n",
       " 'V214',\n",
       " 'V215',\n",
       " 'V220',\n",
       " 'V221',\n",
       " 'V223',\n",
       " 'V224',\n",
       " 'V226',\n",
       " 'V227',\n",
       " 'V229',\n",
       " 'V238',\n",
       " 'V240',\n",
       " 'V241',\n",
       " 'V242',\n",
       " 'V247',\n",
       " 'V250',\n",
       " 'V252',\n",
       " 'V260',\n",
       " 'V261',\n",
       " 'V264',\n",
       " 'V270',\n",
       " 'V281',\n",
       " 'V282',\n",
       " 'V283',\n",
       " 'V284',\n",
       " 'V286',\n",
       " 'V288',\n",
       " 'V290',\n",
       " 'V291',\n",
       " 'V300',\n",
       " 'V305',\n",
       " 'V310',\n",
       " 'V313',\n",
       " 'V314',\n",
       " 'V337',\n",
       " 'id_01',\n",
       " 'id_02',\n",
       " 'id_03',\n",
       " 'id_04',\n",
       " 'id_05',\n",
       " 'id_06',\n",
       " 'id_07',\n",
       " 'id_08',\n",
       " 'id_09',\n",
       " 'id_10',\n",
       " 'id_11',\n",
       " 'id_12',\n",
       " 'id_13',\n",
       " 'id_14',\n",
       " 'id_15',\n",
       " 'id_16',\n",
       " 'id_17',\n",
       " 'id_18',\n",
       " 'id_19',\n",
       " 'id_20',\n",
       " 'id_21',\n",
       " 'id_22',\n",
       " 'id_23',\n",
       " 'id_24',\n",
       " 'id_25',\n",
       " 'id_26',\n",
       " 'id_27',\n",
       " 'id_28',\n",
       " 'id_29',\n",
       " 'id_30',\n",
       " 'id_31',\n",
       " 'id_32',\n",
       " 'id_33',\n",
       " 'id_34',\n",
       " 'id_35',\n",
       " 'id_36',\n",
       " 'id_37',\n",
       " 'id_38',\n",
       " 'DeviceType',\n",
       " 'DeviceInfo',\n",
       " 'day',\n",
       " 'week',\n",
       " 'week2',\n",
       " 'month',\n",
       " 'hour',\n",
       " 'day_of_week',\n",
       " 'TransactionAmt_int',\n",
       " 'TransactionAmt_dec',\n",
       " 'card1__card2',\n",
       " 'card1__card3',\n",
       " 'card1__card4',\n",
       " 'card1__card5',\n",
       " 'card1__addr1',\n",
       " 'card1__addr2',\n",
       " 'card1__dist1',\n",
       " 'card1__dist2',\n",
       " 'card2__card3',\n",
       " 'card2__card4',\n",
       " 'card2__card5',\n",
       " 'card2__addr1',\n",
       " 'card2__addr2',\n",
       " 'card2__dist1',\n",
       " 'card2__dist2',\n",
       " 'card3__card4',\n",
       " 'card3__card5',\n",
       " 'card3__addr1',\n",
       " 'card3__addr2',\n",
       " 'card3__dist1',\n",
       " 'card3__dist2',\n",
       " 'card4__card5',\n",
       " 'card4__addr1',\n",
       " 'card4__addr2',\n",
       " 'card4__dist1',\n",
       " 'card4__dist2',\n",
       " 'card5__addr1',\n",
       " 'card5__addr2',\n",
       " 'card5__dist1',\n",
       " 'card5__dist2',\n",
       " 'addr1__addr2',\n",
       " 'addr1__dist1',\n",
       " 'addr1__dist2',\n",
       " 'addr2__dist1',\n",
       " 'addr2__dist2',\n",
       " 'dist1__dist2',\n",
       " 'P_emaildomain_0',\n",
       " 'P_emaildomain_1',\n",
       " 'P_emaildomain_2',\n",
       " 'R_emaildomain_0',\n",
       " 'R_emaildomain_1',\n",
       " 'R_emaildomain_2',\n",
       " 'operating_system_0',\n",
       " 'operating_system_1',\n",
       " 'operating_system_2',\n",
       " 'operating_system_3',\n",
       " 'browser_0',\n",
       " 'browser_1',\n",
       " 'browser_2',\n",
       " 'browser_3',\n",
       " 'resolution_width',\n",
       " 'resolution_height',\n",
       " 'DeviceInfo_0',\n",
       " 'DeviceInfo_1',\n",
       " 'DeviceInfo_2',\n",
       " 'DeviceInfo_3',\n",
       " 'DeviceInfo_4',\n",
       " 'TransactionAmt_to_mean_card1',\n",
       " 'TransactionAmt_to_mean_card4',\n",
       " 'TransactionAmt_to_std_card1',\n",
       " 'TransactionAmt_to_std_card4',\n",
       " 'id_02_to_mean_card1',\n",
       " 'id_02_to_mean_card4',\n",
       " 'id_02_to_std_card1',\n",
       " 'id_02_to_std_card4',\n",
       " 'D15_to_mean_card1',\n",
       " 'D15_to_mean_card4',\n",
       " 'D15_to_std_card1',\n",
       " 'D15_to_std_card4',\n",
       " 'D15_to_mean_addr1',\n",
       " 'D15_to_mean_addr2',\n",
       " 'D15_to_std_addr1',\n",
       " 'D15_to_std_addr2',\n",
       " 'addr1_FE',\n",
       " 'card1_FE',\n",
       " 'card2_FE',\n",
       " 'card3_FE',\n",
       " 'P_emaildomain_FE',\n",
       " 'card1_addr1',\n",
       " 'card1_addr1_P_emaildomain',\n",
       " 'card1_addr1_FE',\n",
       " 'card1_addr1_P_emaildomain_FE',\n",
       " 'TransactionAmt_card1_mean',\n",
       " 'TransactionAmt_card1_std',\n",
       " 'TransactionAmt_card1_addr1_mean',\n",
       " 'TransactionAmt_card1_addr1_std',\n",
       " 'TransactionAmt_card1_addr1_P_emaildomain_mean',\n",
       " 'TransactionAmt_card1_addr1_P_emaildomain_std',\n",
       " 'D9_card1_mean',\n",
       " 'D9_card1_std',\n",
       " 'D9_card1_addr1_mean',\n",
       " 'D9_card1_addr1_std',\n",
       " 'D9_card1_addr1_P_emaildomain_mean',\n",
       " 'D9_card1_addr1_P_emaildomain_std',\n",
       " 'D11_card1_mean',\n",
       " 'D11_card1_std',\n",
       " 'D11_card1_addr1_mean',\n",
       " 'D11_card1_addr1_std',\n",
       " 'D11_card1_addr1_P_emaildomain_mean',\n",
       " 'D11_card1_addr1_P_emaildomain_std',\n",
       " 'uid',\n",
       " 'outsider15']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          13832_-13.0\n",
       "1            32191_1.0\n",
       "2            37942_1.0\n",
       "3         28240_-111.0\n",
       "4            37442_1.0\n",
       "              ...     \n",
       "590535     44024_153.0\n",
       "590536      1578_182.0\n",
       "590537      7079_182.0\n",
       "590538     48254_160.0\n",
       "590539     17800_182.0\n",
       "Name: uid, Length: 590540, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['uid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW FEATURE\n",
    "df_train['outsider15'] = (np.abs(df_train.D1-df_train.D15)>3).astype('int8')\n",
    "df_test['outsider15'] = (np.abs(df_test.D1-df_test.D15)>3).astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['outsider15'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid_P_emaildomain_ct, uid_dist1_ct, uid_id_02_ct, uid_cents_ct, uid_C13_ct, uid_V314_ct, uid_V136_ct, "
     ]
    }
   ],
   "source": [
    "# FREQUENCY ENCODE UID\n",
    "encode_FE(df_train,df_test,['uid'])\n",
    "# AGGREGATE \n",
    "encode_AG(['TransactionAmt','D4','D9','D10','D15'],['uid'],['mean','std'],fillna=True,usena=True)\n",
    "# AGGREGATE\n",
    "encode_AG(['C'+str(x) for x in range(1,15) if x!=3],['uid'],['mean'],df_train,df_test,fillna=True,usena=True)\n",
    "# AGGREGATE\n",
    "# encode_AG(['M'+str(x) for x in range(1,10)],['uid'],['mean'],fillna=True,usena=True)\n",
    "# AGGREGATE\n",
    "encode_AG2(['P_emaildomain','dist1','id_02','cents'], ['uid'], train_df=df_train, test_df=df_test)\n",
    "# AGGREGATE\n",
    "encode_AG(['C14'],['uid'],['std'],df_train,df_test,fillna=True,usena=True)\n",
    "# AGGREGATE \n",
    "encode_AG2(['C13','V314'], ['uid'], train_df=df_train, test_df=df_test)\n",
    "# AGGREATE \n",
    "encode_AG2(['V136'], ['uid'], train_df=df_train, test_df=df_test)\n",
    "# NEW FEATURE\n",
    "df_train['outsider15'] = (np.abs(df_train.D1-df_train.D15)>3).astype('int8')\n",
    "df_test['outsider15'] = (np.abs(df_test.D1-df_test.D15)>3).astype('int8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns with majority nans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing inf values to nans\n",
    "df_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# removing columns that have more than 90 percent nans\n",
    "col_names_num = df_train[num_features].isna().sum()\n",
    "col_names_cat = df_train[cat_features].isna().sum()\n",
    "\n",
    "drop_num=col_names_num[(col_names_num/df_train.shape[0])>0.9].index\n",
    "drop_cat=col_names_cat[(col_names_cat/df_train.shape[0])>0.9].index\n",
    "\n",
    "for column in df_train.columns:\n",
    "    if column in drop_cat or column in drop_num :\n",
    "        df_train.drop(column, axis=1, inplace=True)\n",
    "        df_test.drop(column, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Nan Values for remaining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating categorical features and numerical features list \n",
    "# which we manually created at start of the notebook\n",
    "num_features_new=[]\n",
    "for i in df_train.columns:\n",
    "    if df_train[i].dtype != 'object':\n",
    "        if i not in drop_num and i!='isFraud':\n",
    "            num_features_new.append(i)\n",
    "cat_features_new=[]\n",
    "for j in df_train.columns:\n",
    "    if df_train[j].dtype == 'object':\n",
    "        if j not in drop_cat and j!='isFraud':\n",
    "            cat_features_new.append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill numerical features Nan with median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "median_values = df_train[num_features_new].median()\n",
    "df_train[num_features_new] = df_train[num_features_new].fillna(median_values)\n",
    "# test data\n",
    "median_values = df_test[num_features_new].median()\n",
    "df_test[num_features_new] = df_test[num_features_new].fillna(median_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Categorical features with % Nan < 1 with interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for columns that have very low nan values we use interpolation to fill nans\n",
    "   based on values above and below of particular nan value\n",
    "   if nans are greater than 1 percent we create a new label for nans as 'missing'.\n",
    "'''\n",
    "for col in cat_features_new:\n",
    "    if df_train[col].isna().sum()==0:\n",
    "        continue\n",
    "    elif df_train[col].isna().sum()<(len(df_train)*0.01):\n",
    "        df_train[col]=df_train[col].astype('category')\n",
    "        df_train[col]= df_train[col].cat.codes.replace(-1,np.nan).interpolate().astype(int).astype('category').cat.rename_categories(df_train[col].cat.categories)\n",
    "    else:\n",
    "        try:\n",
    "            df_train[col]=df_train[col].fillna('missing')\n",
    "        except:\n",
    "            ValueError\n",
    "            df_train[col]=df_train[col].astype('category')\n",
    "            df_train[col]= df_train[col].cat.codes.replace(-1,np.nan).interpolate().astype(int).astype('category').cat.rename_categories(df_train[col].cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test data\n",
    "for col in cat_features_new:\n",
    "    if df_test[col].isna().sum()==0:\n",
    "        continue\n",
    "    elif df_test[col].isna().sum()<(len(df_test)*0.01):\n",
    "        df_test[col]=df_test[col].astype('category')\n",
    "        df_test[col]= df_test[col].cat.codes.replace(-1,np.nan).interpolate().astype(int).astype('category').cat.rename_categories(df_test[col].cat.categories)\n",
    "    else:\n",
    "        try:\n",
    "            df_test[col]=df_test[col].fillna('missing')\n",
    "        except:\n",
    "            ValueError\n",
    "            df_test[col]=df_test[col].astype('category')\n",
    "            df_test[col]= df_test[col].cat.codes.replace(-1,np.nan).interpolate().astype(int).astype('category').cat.rename_categories(df_test[col].cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we keep only common columns that are present in both train and test data\n",
    "list1=df_test.columns\n",
    "list2=df_train.columns\n",
    "common_cols=list(set(list1).intersection(list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 341)\n",
      "(506691, 340)\n",
      "No of Dropped columns which are not common to both train and test 0\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in list2:\n",
    "    if i not in common_cols and i!='isFraud':\n",
    "        df_train.drop(i,axis=1,inplace=True)\n",
    "        count+=1\n",
    "# remove columns in test data that are not in train data\n",
    "for cols in df_test.columns:\n",
    "    if cols not in df_train.columns:\n",
    "        df_test.drop(cols,axis=1,inplace=True)\n",
    "        count+=1\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "print('No of Dropped columns which are not common to both train and test',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OHE and Label encoding for categorical features \n",
    "import bisect\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Categories with low cardinality are one hot encoded\n",
    "   High cardinality features are label encoded'''\n",
    "for cols in cat_features_new:   \n",
    "    df_train[cols]=df_train[cols].astype(str)\n",
    "    le.fit(df_train[cols])\n",
    "    df_train[cols]=le.transform(df_train[cols])\n",
    "    df_test[cols]=df_test[cols].astype(str)\n",
    "    # to handle unknown\n",
    "    le_classes=le.classes_.tolist()\n",
    "    bisect.insort_left(le_classes, 'other')\n",
    "    le.classes_ = le_classes\n",
    "    df_test[cols] = df_test[cols].map(lambda x: 'other' if x not in le.classes_ else x)\n",
    "    df_test[cols]=le.transform(df_test[cols])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 1:Max amount of Transcation by a particular cardnum in 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "df_temp_train=df_train[['uid','month','TransactionAmt']].groupby(['uid','month']).max('TrancsactionAmt').reset_index()\n",
    "df_temp_train.rename({'TransactionAmt':'MAX_amt'},axis=1,inplace=True)\n",
    "df_train=df_train.merge(df_temp_train,on=['uid','month'])\n",
    "\n",
    "# Test data\n",
    "df_temp_test=df_test[['uid','month','TransactionAmt']].groupby(['uid','month']).max('TrancsactionAmt').reset_index()\n",
    "df_temp_test.rename({'TransactionAmt':'MAX_amt'},axis=1,inplace=True)\n",
    "df_test=df_test.merge(df_temp_test,on=['uid','month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 2:Total amount by a particular cardnum in this addr1 in 14 days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "df_temp_train=df_train[['uid','week2','TransactionAmt','addr1']].groupby(['uid','addr1','week2']).sum('TrancsactionAmt').reset_index()\n",
    "df_temp_train.rename({'TransactionAmt':'Total_amt'},axis=1,inplace=True)\n",
    "df_train=df_train.merge(df_temp_train,on=['uid','addr1','week2'])\n",
    "\n",
    "# Test data\n",
    "df_temp_test=df_test[['uid','week2','TransactionAmt','addr1']].groupby(['uid','addr1','week2']).sum('TrancsactionAmt').reset_index()\n",
    "df_temp_test.rename({'TransactionAmt':'Total_amt'},axis=1,inplace=True)\n",
    "df_test=df_test.merge(df_temp_test,on=['uid','addr1','week2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 3:Median amount by a particular cardnum in this addr1 in 30 days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "df_temp_train=df_train[['uid','month','TransactionAmt','addr1']].groupby(['uid','addr1','month']).median('TrancsactionAmt').reset_index()\n",
    "df_temp_train.rename({'TransactionAmt':'Median_amt'},axis=1,inplace=True)\n",
    "df_train=df_train.merge(df_temp_train,on=['uid','addr1','month'])\n",
    "\n",
    "# Test data\n",
    "df_temp_test=df_test[['uid','month','TransactionAmt','addr1']].groupby(['uid','addr1','month']).median('TrancsactionAmt').reset_index()\n",
    "df_temp_test.rename({'TransactionAmt':'Median_amt'},axis=1,inplace=True)\n",
    "df_test=df_test.merge(df_temp_test,on=['uid','addr1','month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 4:Total Number of transactions for a cardnum in a given day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "df_temp_train=df_train[['uid','day','TransactionID']].groupby(['uid','day']).count().reset_index()\n",
    "df_temp_train.rename({'TransactionID':'#tran_1day'},axis=1,inplace=True)\n",
    "df_train=df_train.merge(df_temp_train,on=['uid','day'])\n",
    "\n",
    "# Test data\n",
    "df_temp_test=df_test[['uid','day','TransactionID']].groupby(['uid','day']).count().reset_index()\n",
    "df_temp_test.rename({'TransactionID':'#tran_1day'},axis=1,inplace=True)\n",
    "df_test=df_test.merge(df_temp_test,on=['uid','day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 5:Actual/Average amount by this cardnum in 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Frist we are grouping based on card num and month of transaction we obtain \n",
    "   No of transaction done by the card in a given month and Actual Amount of transaction\n",
    "   Then we obtain Average amount of transaction by dividing actual transactionamt by no of transactions\n",
    "''' \n",
    "# Train data\n",
    "df_temp_train=df_train[['uid','month','TransactionID','TransactionAmt']].groupby(['uid','month']).agg({'TransactionID':'count','TransactionAmt':'sum'}).reset_index()\n",
    "df_temp_train.rename({'TransactionID':'count','TransactionAmt':'Act_amt'},axis=1,inplace=True)\n",
    "df_temp_train['Average_amt']=df_temp_train['Act_amt'].values/df_temp_train['count'].values\n",
    "df_temp_train['Act_amt/Avg_amt']=df_temp_train['Act_amt'].values/df_temp_train['Average_amt'].values\n",
    "df_train=df_train.merge(df_temp_train.drop(['count','Act_amt','Average_amt'],axis=1),on=['uid','month'])\n",
    "\n",
    "# Test data\n",
    "df_temp_test=df_test[['uid','month','TransactionID','TransactionAmt']].groupby(['uid','month']).agg({'TransactionID':'count','TransactionAmt':'sum'}).reset_index()\n",
    "df_temp_test.rename({'TransactionID':'count','TransactionAmt':'Act_amt'},axis=1,inplace=True)\n",
    "df_temp_test['Average_amt']=df_temp_test['Act_amt'].values/df_temp_test['count'].values\n",
    "df_temp_test['Act_amt/Avg_amt']=df_temp_test['Act_amt'].values/df_temp_test['Average_amt'].values\n",
    "df_test=df_test.merge(df_temp_test.drop(['count','Act_amt','Average_amt'],axis=1),on=['uid','month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 6:No of transactions by this cardnum in this addr2 in 1 day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "df_temp_train=df_train[['uid','day','TransactionID','addr2']].groupby(['uid','day','addr2']).agg({'TransactionID':'count'}).reset_index()\n",
    "df_temp_train.rename({'TransactionID':'#tran_1day_addr2'},axis=1,inplace=True)\n",
    "df_train=df_train.merge(df_temp_train,on=['uid','day','addr2'])\n",
    "\n",
    "# Test data\n",
    "df_temp_test=df_test[['uid','day','TransactionID','addr2']].groupby(['uid','day','addr2']).agg({'TransactionID':'count'}).reset_index()\n",
    "df_temp_test.rename({'TransactionID':'#tran_1day_addr2'},axis=1,inplace=True)\n",
    "df_test=df_test.merge(df_temp_test,on=['uid','day','addr2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 7:Max amount by this cardnum in this addr2 in 14 day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "df_temp_train=df_train[['uid','week2','TransactionAmt','addr2']].groupby(['uid','week2','addr2']).agg({'TransactionAmt':'max'}).reset_index()\n",
    "df_temp_train.rename({'TransactionAmt':'Maxamt_14day_addr2'},axis=1,inplace=True)\n",
    "df_train=df_train.merge(df_temp_train,on=['uid','week2','addr2'])\n",
    "\n",
    "# Test data\n",
    "df_temp_test=df_test[['uid','week2','TransactionAmt','addr2']].groupby(['uid','week2','addr2']).agg({'TransactionAmt':'max'}).reset_index()\n",
    "df_temp_test.rename({'TransactionAmt':'Maxamt_14day_addr2'},axis=1,inplace=True)\n",
    "df_test=df_test.merge(df_temp_test,on=['uid','week2','addr2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 8:Median amount by this cardnum in this addr1 in 30 day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "df_temp_train=df_train[['uid','month','TransactionAmt','addr1']].groupby(['uid','month','addr1']).agg({'TransactionAmt':'median'}).reset_index()\n",
    "df_temp_train.rename({'TransactionAmt':'Medianamt_30day_addr1'},axis=1,inplace=True)\n",
    "df_train=df_train.merge(df_temp_train,on=['uid','month','addr1'])\n",
    "\n",
    "# Test data\n",
    "df_temp_test=df_test[['uid','month','TransactionAmt','addr1']].groupby(['uid','month','addr1']).agg({'TransactionAmt':'median'}).reset_index()\n",
    "df_temp_test.rename({'TransactionAmt':'Medianamt_30day_addr1'},axis=1,inplace=True)\n",
    "df_test=df_test.merge(df_temp_test,on=['uid','month','addr1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 9:Median amount by this cardnum in 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "df_temp_train=df_train[['uid','month','TransactionAmt']].groupby(['uid','month']).agg({'TransactionAmt':'median'}).reset_index()\n",
    "df_temp_train.rename({'TransactionAmt':'Medianamt_30day'},axis=1,inplace=True)\n",
    "df_tran=df_train.merge(df_temp_train,on=['uid','month'])\n",
    "\n",
    "# Test data\n",
    "df_temp_test=df_test[['uid','month','TransactionAmt']].groupby(['uid','month']).agg({'TransactionAmt':'median'}).reset_index()\n",
    "df_temp_test.rename({'TransactionAmt':'Medianamt_30day'},axis=1,inplace=True)\n",
    "df_test=df_test.merge(df_temp_test,on=['uid','month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 10:Total amount by this cardnum in this addr1 in 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "df_temp_train=df_train[['uid','month','TransactionAmt','addr1']].groupby(['uid','month','addr1']).agg({'TransactionAmt':'sum'}).reset_index()\n",
    "df_temp_train.rename({'TransactionAmt':'Totalamt_30day_addr1'},axis=1,inplace=True)\n",
    "df_train=df_train.merge(df_temp_train,on=['uid','month','addr1'])\n",
    "\n",
    "# Test data\n",
    "df_temp_test=df_test[['uid','month','TransactionAmt','addr1']].groupby(['uid','month','addr1']).agg({'TransactionAmt':'sum'}).reset_index()\n",
    "df_temp_test.rename({'TransactionAmt':'Totalamt_30day_addr1'},axis=1,inplace=True)\n",
    "df_test=df_test.merge(df_temp_test,on=['uid','month','addr1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 11:Log TransactionAmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['log_amt']=np.log(df_train['TransactionAmt'].values)\n",
    "df_test['log_amt'] =np.log(df_test['TransactionAmt'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Periodic feature from Vonmises Distribution\n",
    "1) When using the aggregated features there is still some information that is not completely captured by those features.In particular when we are interested in analyzing the time of the transaction.\n",
    "2) we use a new method for extracting periodic features in order to estimate if the time of a new transaction is with in the confidence interval of the previous transaction times.\n",
    "3) The motivation is that a customer is expected to make transactions at similar hours. The proposed methodology is based on analyzing the periodic behavior of a transactiontime,using the vonMises distribution.\n",
    "4) The vonMises distribution,also known as the periodic normal distribution,is a distribution of a wrappe dnormal distributed variable across a circle.\n",
    "5) In particular we are interested in calculating a confidence interval(CI) for the time of a transaction.For doing that initially we select a set of transactions made by the same client in the last 7 days.\n",
    "6) Afterwards,the probability distribution function of the time of the set of transactions is calculated with the help of vonmises module in scipy.stats\n",
    "7) Vonmises.interval gives us the required confidence interval for present transaction.\n",
    "8) Expected time of a transaction Using the confidence interval,a transaction can be flag normal or suspicious,depending whether or not the time of the transaction is with in the confidence interval.\n",
    "9) Then, using the estimated distribution,a new set of features can be extracted,ie.,a binaryfeature if a new transaction time is with in the confidence interval range with probability α(confidence score).\n",
    "**Reference**<br>\n",
    "https://albahnsen.github.io/files/Feature%20Engineering%20Strategies%20for%20Credit%20Card%20Fraud%20Detection_published.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time=df_train[['TransactionID','hour','uid','day','week']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for index(start from zero to len of dataframe),i is a list [hour,week]\n",
    "        if index >1:\n",
    "            if past_week(we obtained from else condition)==present_week(i[index])\n",
    "                we fit all transactions occured till now(not including present transaction) in this week \n",
    "                and get confidence interval for the present transaction.\n",
    "                we append at what hour transaction occured in temp list\n",
    "                we store the present transaction week information in past_week \n",
    "\n",
    "            else:(i.e new transaction doesn't belong to pastweek)\n",
    "                we empty all the details of previous transactions \n",
    "                fit with the new data get confidence interval\n",
    "                we append at what hour transaction occured in temp list\n",
    "                we store the present transaction week information in past_week \n",
    "            \n",
    "        else:\n",
    "            frist transaction for a given card doesn't have any previous transaction\n",
    "            so we directly fit the data and get kappa,loc (parameters of vonmises distribution)\n",
    "            use them to get confidence interval for index=0 transaction.\n",
    "            we append at what hour transaction occured in temp list\n",
    "            we store the present transaction week information in past_week \n",
    "'''\n",
    "def get_CI_and_binary_feature(df3):\n",
    "    confidence=[]\n",
    "    temp=[]\n",
    "    for index,i in enumerate(df3[['hour','week']].values):\n",
    "        if index>1:\n",
    "            if past_week==i[1]:\n",
    "                kappa,loc,_=vonmises.fit(temp,fscale=1)\n",
    "                confidence.append(np.round(vonmises.interval(0.9,kappa,loc=loc, scale=1)))\n",
    "                temp.append(i[0])\n",
    "                past_week=i[1]\n",
    "            else:\n",
    "                temp=[]\n",
    "                kappa,loc,_=vonmises.fit(i[0],fscale=1)\n",
    "                confidence.append(np.round(vonmises.interval(0.9,kappa,loc=loc, scale=1)))\n",
    "                temp.append(i[0])\n",
    "                past_week=i[1]\n",
    "        else:\n",
    "            kappa,loc,_=vonmises.fit(i[0],fscale=1)\n",
    "            confidence.append(np.round(vonmises.interval(0.9,kappa,loc=loc, scale=1)))\n",
    "            past_week=i[1]\n",
    "            temp.append(i[0])\n",
    "    # create a new column with name ConfidenceInterval\n",
    "    df3['ConfidenceInterval']=confidence\n",
    "    # create a new binary feature\n",
    "    ''' for item [hour,confidenceinterval]\n",
    "            if present transaction hour lies with in confidence interval \n",
    "            we return true else false '''  \n",
    "    binary_feature=[]\n",
    "    for item in df3[['hour','ConfidenceInterval']].values:\n",
    "        if item[0]>=item[1][0] and item[0]<=item[1][1]:\n",
    "            binary_feature.append(True)\n",
    "        else:\n",
    "            binary_feature.append(False) \n",
    "    df3['binary_feature']=binary_feature   \n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_feature={}\n",
    "for index,i in enumerate(df_time['card_id'].unique()):\n",
    "    df3=df_time.loc[df_time['card_id']==i]\n",
    "    df3=get_CI_and_binary_feature(df3)\n",
    "    if index==0:\n",
    "        dict_feature['TransactionID']=df3['TransactionID'].values\n",
    "        dict_feature['ConfidenceInterval']=df3['ConfidenceInterval'].values\n",
    "        dict_feature['binary_feature']=df3['binary_feature'].values\n",
    "    else:\n",
    "        dict_feature['TransactionID']=np.append(dict_feature['TransactionID'],df3['TransactionID'].values)\n",
    "        dict_feature['ConfidenceInterval']=np.append(dict_feature['ConfidenceInterval'],df3['ConfidenceInterval'].values)\n",
    "        dict_feature['binary_feature']=np.append(dict_feature['binary_feature'],df3['binary_feature'].values)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>ConfidenceInterval</th>\n",
       "      <th>binary_feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000</td>\n",
       "      <td>[-0.  0.]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3023492</td>\n",
       "      <td>[12. 12.]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3026562</td>\n",
       "      <td>[6. 6.]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3033460</td>\n",
       "      <td>[4. 9.]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3042544</td>\n",
       "      <td>[22. 22.]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID ConfidenceInterval  binary_feature\n",
       "0        2987000          [-0.  0.]            True\n",
       "1        3023492          [12. 12.]            True\n",
       "2        3026562            [6. 6.]           False\n",
       "3        3033460            [4. 9.]           False\n",
       "4        3042544          [22. 22.]            True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_binary=pd.read_csv('vonmisses_train.csv')\n",
    "df_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.merge(df_binary,on='TransactionID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time=df_test[['TransactionID','hour','card_id','day','week']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_feature={}\n",
    "for index,i in enumerate(df_time['card_id'].unique()):\n",
    "    df3=df_time.loc[df_time['card_id']==i]\n",
    "    df3=get_CI_and_binary_feature(df3)\n",
    "    if index==0:\n",
    "        dict_feature['TransactionID']=df3['TransactionID'].values\n",
    "        dict_feature['ConfidenceInterval']=df3['ConfidenceInterval'].values\n",
    "        dict_feature['binary_feature']=df3['binary_feature'].values\n",
    "    else:\n",
    "        dict_feature['TransactionID']=np.append(dict_feature['TransactionID'],df3['TransactionID'].values)\n",
    "        dict_feature['ConfidenceInterval']=np.append(dict_feature['ConfidenceInterval'],df3['ConfidenceInterval'].values)\n",
    "        dict_feature['binary_feature']=np.append(dict_feature['binary_feature'],df3['binary_feature'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>ConfidenceInterval</th>\n",
       "      <th>binary_feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3663549</td>\n",
       "      <td>[-0.  0.]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3665539</td>\n",
       "      <td>[22. 22.]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3671801</td>\n",
       "      <td>[ 8. 14.]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3673579</td>\n",
       "      <td>[12. 17.]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3687976</td>\n",
       "      <td>[15. 15.]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID ConfidenceInterval  binary_feature\n",
       "0        3663549          [-0.  0.]            True\n",
       "1        3665539          [22. 22.]            True\n",
       "2        3671801          [ 8. 14.]           False\n",
       "3        3673579          [12. 17.]            True\n",
       "4        3687976          [15. 15.]            True"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_binary=pd.read_csv('vonmisses_test.csv')\n",
    "df_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df_test.merge(df_binary,on='TransactionID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 353)\n",
      "(506691, 353)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to pickle files for faster loading\n",
    "df_train.to_pickle('df_train_fe.pkl')\n",
    "df_test.to_pickle('df_test_fe.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vonmises distribution\n",
    "1) Vonmises distribution help to understand periodic nature of time features.\n",
    "2) Here using Binary feature we are predicting present transaction of a given creditcard follows previous transactions distribution or not.\n",
    "3) Time span of 7 days <br>\n",
    "**The above process is taking more than 30 hrs to complete for entire dataset**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffc07624923affceafa6fae0f78aaad727ebda5598769363d4c8d76f29989191"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
